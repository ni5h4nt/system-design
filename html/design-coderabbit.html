<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Design CodeRabbit ‚Äî Worked Example</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=DM+Sans:ital,wght@0,400;0,500;0,600;0,700&family=Fraunces:ital,opsz,wght@0,9..144,700;1,9..144,400&display=swap" rel="stylesheet">
<style>
:root{--bg:#0e1117;--surface:#161b22;--surface-raised:#1c2129;--border:#2d333b;--border-light:#373e47;--text:#e6edf3;--text-muted:#8b949e;--text-dim:#6e7681;--accent-blue:#58a6ff;--accent-green:#3fb950;--accent-orange:#d29922;--accent-red:#f85149;--accent-purple:#bc8cff;--accent-cyan:#39d2c0;--accent-yellow:#e3b341;--phase1:#58a6ff;--phase2:#d29922;--phase3:#3fb950;--phase4:#f85149;--phase5:#bc8cff;--phase6:#39d2c0;--nav-width:270px;--font-body:'DM Sans',-apple-system,sans-serif;--font-mono:'JetBrains Mono',monospace;--font-display:'Fraunces',Georgia,serif}*{margin:0;padding:0;box-sizing:border-box}html{scroll-behavior:smooth;scroll-padding-top:24px}body{font-family:var(--font-body);background:var(--bg);color:var(--text);font-size:14px;line-height:1.6}
nav{position:fixed;top:0;left:0;width:var(--nav-width);height:100vh;background:var(--surface);border-right:1px solid var(--border);padding:24px 0;overflow-y:auto;z-index:100;display:flex;flex-direction:column}nav .logo{padding:0 20px 20px;border-bottom:1px solid var(--border);margin-bottom:16px}nav .logo h1{font-family:var(--font-display);font-size:18px;font-weight:700;color:var(--text);letter-spacing:-0.02em;line-height:1.3}nav .logo span{display:block;font-family:var(--font-body);font-size:11px;color:var(--text-dim);margin-top:4px;text-transform:uppercase;letter-spacing:0.08em}.nav-section-label{font-size:10px;font-weight:600;text-transform:uppercase;letter-spacing:0.1em;color:var(--text-dim);padding:12px 20px 6px}nav a{display:flex;align-items:center;gap:10px;padding:7px 20px;color:var(--text-muted);text-decoration:none;font-size:13px;font-weight:500;transition:all .15s;border-left:2px solid transparent}nav a:hover{color:var(--text);background:var(--surface-raised)}nav a.active{color:var(--text);border-left-color:var(--accent-blue);background:rgba(88,166,255,.06)}.nav-dot{width:7px;height:7px;border-radius:50%;flex-shrink:0}.nav-time{margin-left:auto;font-family:var(--font-mono);font-size:10px;color:var(--text-dim);background:var(--surface-raised);padding:1px 6px;border-radius:3px}
main{margin-left:var(--nav-width);padding:32px 48px 120px;max-width:960px}
.phase{margin-bottom:40px;border:1px solid var(--border);border-radius:10px;overflow:hidden;background:var(--surface)}.phase-header{display:flex;align-items:center;gap:14px;padding:16px 20px;cursor:pointer;user-select:none;transition:background .15s}.phase-header:hover{background:var(--surface-raised)}.phase-number{font-family:var(--font-mono);font-size:11px;font-weight:600;padding:3px 8px;border-radius:4px;color:var(--bg);flex-shrink:0}.phase-title{font-family:var(--font-display);font-size:17px;font-weight:700;flex:1}.phase-time{font-family:var(--font-mono);font-size:12px;color:var(--text-muted);flex-shrink:0}.phase-chevron{width:20px;height:20px;color:var(--text-dim);transition:transform .25s ease;flex-shrink:0}.phase.collapsed .phase-chevron{transform:rotate(-90deg)}.phase.collapsed .phase-body{display:none}.phase-body{padding:0 20px 20px;border-top:1px solid var(--border)}
.callout{margin:14px 0;padding:12px 16px;border-radius:0 6px 6px 0;font-size:13px;line-height:1.6}.callout.goal{background:rgba(88,166,255,.05);border-left:3px solid var(--accent-blue);color:var(--text-muted)}.callout.goal strong{color:var(--accent-blue)}.callout.say{background:rgba(63,185,80,.06);border-left:3px solid var(--accent-green);color:var(--text-muted)}.callout.say::before{content:'üó£Ô∏è '}.callout.tip{background:rgba(210,153,34,.06);border-left:3px solid var(--accent-orange);color:var(--text-muted)}.callout.tip::before{content:'üí° '}.callout.decision{background:rgba(248,81,73,.05);border-left:3px solid var(--accent-red);color:var(--text-muted)}.callout.decision::before{content:'‚öñÔ∏è '}.callout code{background:rgba(255,255,255,.06);padding:1px 5px;border-radius:3px;font-family:var(--font-mono);font-size:12px}
.sub{font-size:14px;font-weight:700;color:var(--accent-cyan);margin:20px 0 8px;padding-bottom:6px;border-bottom:1px solid var(--border)}
.items{list-style:none;margin:10px 0}.items li{position:relative;padding:5px 0 5px 22px;font-size:13.5px;line-height:1.55;color:var(--text-muted)}.items li::before{content:'‚Üí';position:absolute;left:2px;color:var(--text-dim);font-family:var(--font-mono);font-size:12px}.items li strong{color:var(--text);font-weight:600}
table{width:100%;border-collapse:collapse;font-size:12.5px;margin:12px 0}thead th{text-align:left;font-size:10px;text-transform:uppercase;letter-spacing:.08em;color:var(--text-dim);padding:8px 10px;border-bottom:1px solid var(--border-light);font-weight:600}tbody td{padding:8px 10px;border-bottom:1px solid var(--border);vertical-align:top;line-height:1.5;color:var(--text-muted)}tbody tr:last-child td{border-bottom:none}tbody td:first-child{font-weight:600;color:var(--text);font-family:var(--font-mono);font-size:11.5px;white-space:nowrap}
.est-grid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:14px 0}.est-card{padding:12px 14px;border:1px solid var(--border);border-radius:8px;background:var(--surface-raised)}.est-card .label{font-size:10px;text-transform:uppercase;letter-spacing:.08em;color:var(--text-dim);margin-bottom:4px}.est-card .value{font-family:var(--font-mono);font-size:18px;font-weight:600;color:var(--accent-yellow)}.est-card .detail{font-size:11.5px;color:var(--text-dim);margin-top:4px;line-height:1.4}
.schema{background:var(--surface-raised);border:1px solid var(--border);border-radius:8px;padding:14px 16px;margin:12px 0;font-family:var(--font-mono);font-size:12px;line-height:1.7;color:var(--text-muted);overflow-x:auto;white-space:pre}.schema .table-name{color:var(--accent-cyan);font-weight:600}.schema .pk{color:var(--accent-yellow)}.schema .fk{color:var(--accent-purple)}.schema .type{color:var(--text-dim)}.schema .comment{color:var(--text-dim);font-style:italic}
.flow-diagram{background:var(--surface-raised);border:1px solid var(--border);border-radius:8px;padding:20px;margin:14px 0;font-family:var(--font-mono);font-size:12px;line-height:2;color:var(--text-muted);overflow-x:auto;white-space:pre;text-align:center}.flow-diagram .highlight{color:var(--accent-cyan);font-weight:600}.flow-diagram .arrow{color:var(--text-dim)}.flow-diagram .label{color:var(--accent-orange);font-size:10px}
.comp-grid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:12px 0}.comp-card{padding:12px 14px;border:1px solid var(--border);border-radius:8px;background:var(--surface-raised)}.comp-card h4{font-size:13px;font-weight:600;color:var(--text);margin-bottom:6px;display:flex;align-items:center;gap:6px}.comp-card h4 .tag{font-family:var(--font-mono);font-size:9px;padding:2px 6px;border-radius:3px;font-weight:600}.comp-card ul{list-style:none;font-size:12px;color:var(--text-muted);line-height:1.55}.comp-card ul li::before{content:'‚Ä¢ ';color:var(--text-dim)}
.failure-row{display:flex;gap:8px;margin:6px 0;font-size:12.5px;align-items:flex-start}.failure-row .scenario{color:var(--accent-red);font-weight:600;min-width:210px;flex-shrink:0}.failure-row .mitigation{color:var(--text-muted)}
@media(max-width:900px){nav{display:none}main{margin-left:0;padding:20px 16px 80px}.est-grid,.comp-grid{grid-template-columns:1fr}}@media print{nav{display:none}main{margin-left:0;max-width:100%}.phase.collapsed .phase-body{display:block}}

/* SVG Diagram Styles */
.svg-diagram{margin:14px 0;border:1px solid var(--border);border-radius:8px;background:var(--surface-raised);overflow:hidden;position:relative}
.svg-diagram svg{display:block;width:100%;height:auto}
.svg-diagram .dia-title{position:absolute;top:10px;right:14px;font-family:var(--font-mono);font-size:9px;letter-spacing:.08em;text-transform:uppercase;color:var(--text-dim);opacity:.6}
.svg-node{transition:filter .2s ease}.svg-node:hover{filter:brightness(1.25)}
@keyframes fadeInUp{from{opacity:0;transform:translateY(6px)}to{opacity:1;transform:translateY(0)}}
.svg-diagram[data-anim] .svg-node{animation:fadeInUp .4s ease both}
</style>
</head>
<body>
<nav>
  <div class="logo"><h1>Design CodeRabbit</h1><span>AI Code Review Platform ¬∑ 75 min</span></div>
  <div class="nav-section-label">Interview Phases</div>
  <a href="#p1"><span class="nav-dot" style="background:var(--phase1)"></span>Clarify & Scope<span class="nav-time">5-7m</span></a>
  <a href="#p2"><span class="nav-dot" style="background:var(--phase2)"></span>Estimation<span class="nav-time">3-5m</span></a>
  <a href="#p3"><span class="nav-dot" style="background:var(--phase3)"></span>High-Level Design<span class="nav-time">8-12m</span></a>
  <a href="#p4"><span class="nav-dot" style="background:var(--phase4)"></span>Deep Dives<span class="nav-time">25-30m</span></a>
  <a href="#p5"><span class="nav-dot" style="background:var(--phase5)"></span>Cross-Cutting<span class="nav-time">10-12m</span></a>
  <a href="#p6"><span class="nav-dot" style="background:var(--phase6)"></span>Wrap-Up<span class="nav-time">3-5m</span></a>
  <div class="nav-section-label">Deep Dives</div>
  <a href="#dd-pipeline">Review Pipeline & Context Engine</a>
  <a href="#dd-sandbox">Sandboxed Execution</a>
  <a href="#dd-learning">Learning & Feedback Loop</a>
  <a href="#dd-data">Data Model & Storage</a>
  <a href="#p7"><span class="nav-dot" style="background:var(--accent-cyan)"></span>Interview Q&amp;A<span class="nav-time">Practice</span></a>
</nav>
<main>

<!-- P1 -->
<div class="phase" id="p1">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase1)">01</span>
    <span class="phase-title">Clarify the Problem & Scope</span><span class="phase-time">5‚Äì7 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">
    <div class="callout say">"We're designing an AI-powered code review platform like CodeRabbit. When a developer opens a pull request on GitHub or GitLab, our system automatically reviews the code changes ‚Äî providing a summary, file-by-file analysis, bug detection, security checks, and inline suggestions ‚Äî then posts the review as PR comments. The system must understand the FULL codebase context, not just the diff, and learn from team feedback over time."</div>

    <div class="sub">Questions I'd Ask</div>
    <ul class="items">
            <li><strong>What outcome matters most?</strong> <em>‚Üí Review quality (bugs caught before merge / total bugs) balanced against false positive rate. A tool that flags real bugs is invaluable. A tool that floods PRs with noise gets disabled. Secondary: time-to-first-review (developers shouldn't wait for human reviewer when AI can start immediately). This shapes the model cascade: prefer precision over recall ‚Äî it's better to miss a minor issue than to cry wolf.</em></li>
      <li><strong>Trigger mechanism?</strong> PR creation/update only, or also IDE and CLI? <em>‚Üí PR webhook is primary. IDE and CLI as secondary surfaces.</em></li>
      <li><strong>Platforms?</strong> GitHub, GitLab, Bitbucket? <em>‚Üí All three. Abstracted behind a platform adapter layer.</em></li>
      <li><strong>What does "context-aware" mean specifically?</strong> <em>‚Üí The review should understand: the diff, the full codebase (dependencies, call graph), linked issues (Jira/Linear), past PRs, team coding standards, and previous feedback from this team.</em></li>
      <li><strong>Static analysis too, or just LLM?</strong> <em>‚Üí Both. 40+ linters/SAST tools run alongside LLM review. Results synthesized into unified feedback.</em></li>
      <li><strong>Interactive?</strong> Can devs reply to review comments? <em>‚Üí Yes ‚Äî conversational. Dev replies, AI responds, and can learn from feedback.</em></li>
      <li><strong>Scale?</strong> <em>‚Üí ~100K repositories, ~500K PRs/day, ~2M review comments posted/day.</em></li>
    </ul>

    <div class="sub">Agreed Scope</div>
    <table>
      <thead><tr><th>In Scope</th><th>Out of Scope</th></tr></thead>
      <tbody>
        <tr><td>Webhook ingestion (GitHub/GitLab/Bitbucket)</td><td>IDE inline review (mention as extension)</td></tr>
        <tr><td>Codebase cloning & context assembly</td><td>Code fix auto-application</td></tr>
        <tr><td>LLM-powered code review pipeline</td><td>CI/CD pipeline integration</td></tr>
        <tr><td>Static analysis tool orchestration (40+ linters)</td><td>LLM training / fine-tuning</td></tr>
        <tr><td>PR comment posting (summary + inline)</td><td>Billing / subscription</td></tr>
        <tr><td>Conversational follow-up in PR threads</td><td>Code generation / auto-fix PRs</td></tr>
        <tr><td>Learning from team feedback</td><td></td></tr>
        <tr><td>Code graph / dependency analysis</td><td></td></tr>
      </tbody>
    </table>

    <div class="sub">Core Use Cases</div>
    <ul class="items">
      <li><strong>UC1:</strong> Dev opens PR ‚Üí within 90 seconds, CodeRabbit posts a review: summary of changes, walkthrough, architecture diagram, file-by-file comments with inline suggestions.</li>
      <li><strong>UC2:</strong> Dev pushes new commits to PR ‚Üí incremental review of only the new changes, referencing the prior review context.</li>
      <li><strong>UC3:</strong> Dev replies to a CodeRabbit comment "we prefer 2-space indentation" ‚Üí system learns this preference, applies to all future reviews for this repo.</li>
      <li><strong>UC4:</strong> CodeRabbit detects a race condition by analyzing the dependency graph, not just the diff ‚Äî something a diff-only reviewer would miss.</li>
    </ul>

    <div class="sub">Non-Functional Requirements</div>
    <ul class="items">
      <li><strong>Latency:</strong> Full review posted within 90 seconds of PR event. First comment (summary) within 30s.</li>
      <li><strong>Accuracy > speed:</strong> False positives erode trust fast. Better to post fewer, high-quality comments than flood the PR with noise.</li>
      <li><strong>Security:</strong> Customer code is cloned into an ephemeral sandbox, processed, then destroyed. Zero persistence after review. No code used for model training.</li>
      <li><strong>Scalability:</strong> Handle PR spike during business hours (8am‚Äì6pm per timezone = rolling peak).</li>
      <li><strong>Reliability:</strong> Webhook must ACK within 50ms. Actual review processing is async. GitHub will retry if we timeout.</li>
    </ul>

    <div class="callout tip">The defining tension of this system: the review must understand the ENTIRE codebase (not just the diff), but must complete within 90 seconds. Context assembly ‚Äî deciding what to include and what to skip ‚Äî is the hardest problem. More context = better review but slower + more expensive. The architecture is a CONTEXT ENGINEERING problem.</div>
  </div>
</div>

<!-- P2 -->
<div class="phase" id="p2">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase2);color:var(--bg)">02</span>
    <span class="phase-title">Back-of-the-Envelope Estimation</span><span class="phase-time">3‚Äì5 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">
    <div class="est-grid">
      <div class="est-card"><div class="label">Repositories</div><div class="value">~100K</div><div class="detail">Active repos with CodeRabbit installed</div></div>
      <div class="est-card"><div class="label">PRs / Day</div><div class="value">~500K</div><div class="detail">~6/sec avg, ~50/sec peak (business hours)</div></div>
      <div class="est-card"><div class="label">Webhook Events / Day</div><div class="value">~2M</div><div class="detail">PRs + pushes + comments + replies. ~23/sec avg, ~200/sec peak.</div></div>
      <div class="est-card"><div class="label">Review Processing Time</div><div class="value">60‚Äì90 sec</div><div class="detail">Clone + context assembly + linters + LLM + post comments</div></div>
      <div class="est-card"><div class="label">Concurrent Reviews</div><div class="value">~500‚Äì3K</div><div class="detail">50 PRs/sec √ó 90s processing = ~4,500 concurrent at peak</div></div>
      <div class="est-card"><div class="label">LLM Calls / Review</div><div class="value">~5‚Äì15</div><div class="detail">Summary + per-file reviews (batched) + verification pass. ~5M LLM calls/day.</div></div>
      <div class="est-card"><div class="label">Linter Executions / Review</div><div class="value">~10‚Äì20</div><div class="detail">40+ tools, but only relevant ones per language. ~5M linter runs/day.</div></div>
      <div class="est-card"><div class="label">Code Cloned / Day</div><div class="value">~50 TB transient</div><div class="detail">500K PRs √ó avg 100MB repo. Ephemeral ‚Äî destroyed after review.</div></div>
    </div>

    <div class="callout decision"><strong>Key insight #1:</strong> 4,500 concurrent reviews at peak, each needing an isolated sandbox with a cloned repo. This is a compute orchestration problem. Serverless / container-per-review is the natural fit ‚Äî spin up, process, tear down.</div>

    <div class="callout decision"><strong>Key insight #2:</strong> The webhook gateway must ACK in 50ms, but reviews take 90 seconds. This FORCES an event-driven architecture ‚Äî immediate ACK, async processing via queue.</div>

    <div class="callout decision"><strong>Key insight #3:</strong> 5M LLM calls/day is the cost bottleneck. Not every file needs the most expensive model. A MODEL CASCADE (cheap model for simple files, expensive model for complex logic) can cut costs 5-10√ó without quality loss.</div>
  </div>
</div>

<!-- P3 -->
<div class="phase" id="p3">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase3);color:var(--bg)">03</span>
    <span class="phase-title">High-Level Design</span><span class="phase-time">8‚Äì12 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">
    <div class="callout say">"The architecture has three distinct layers: a thin webhook gateway that ACKs immediately, a message queue that buffers reviews, and a fleet of sandboxed review workers that do the actual analysis. Let me draw the full flow."</div>

    
    <div class="sub">Key Architecture Decisions</div>
    <div class="callout say">"Here's WHY I chose each technology ‚Äî mapping requirements to tradeoffs. Every choice has a rejected alternative and a consequence."</div>
    <table>
      <thead><tr><th style="width:22%">Requirement</th><th style="width:20%">Decision</th><th style="width:42%">Why (and what was rejected)</th><th style="width:16%">Consistency</th></tr></thead>
      <tbody>
      <tr><td>Review posted within 90 seconds of PR</td><td style="color:var(--accent-cyan);font-weight:500">Async pipeline: Kafka ‚Üí clone ‚Üí analyze ‚Üí post</td><td>Webhook triggers async flow. Clone + static analysis + AI review parallelized where possible. Synchronous webhook processing would timeout.</td><td>‚Äî</td></tr>
      <tr><td>Customer code security: never persisted</td><td style="color:var(--accent-cyan);font-weight:500">Ephemeral sandboxes, destroyed after review</td><td>Code cloned into ephemeral container, deleted after review completes. No persistent storage of customer code reduces breach blast radius.</td><td>‚Äî</td></tr>
      <tr><td>False positive rate must decrease over time</td><td style="color:var(--accent-cyan);font-weight:500">Feedback loop: developer reactions ‚Üí per-repo tuning</td><td>Thumbs up/down on comments feed into per-repo learned preferences. Comment patterns consistently dismissed get suppressed.</td><td>‚Äî</td></tr>
      <tr><td>Multi-language: Python, JS, Go, Rust, etc.</td><td style="color:var(--accent-cyan);font-weight:500">LLM-based review (not language-specific rules)</td><td>LLM understands all languages. Rule-based approach requires per-language rule sets ‚Äî doesn't scale to 20+ languages.</td><td>‚Äî</td></tr>
      <tr><td>Context beyond the diff: understand the whole codebase</td><td style="color:var(--accent-cyan);font-weight:500">Vector embeddings of repository (LanceDB)</td><td>Index existing codebase for semantic search. Review comments reference patterns elsewhere in the repo, not just the changed lines.</td><td>‚Äî</td></tr>
      </tbody>
    </table>

    <div class="sub">Major Components</div>
    <div class="svg-diagram" data-anim>
  <span class="dia-title">High-Level Architecture</span>
  <svg viewBox="0 0 780 556" xmlns="http://www.w3.org/2000/svg" style="font-family:'DM Sans',sans-serif">
    <defs>
      <marker id="topo_8063" markerWidth="7" markerHeight="5" refX="7" refY="2.5" orient="auto"><path d="M0,0 L7,2.5 L0,5" fill="none" stroke="#6e7681" stroke-width="1"/></marker>
      <marker id="topo_8063h" markerWidth="7" markerHeight="5" refX="7" refY="2.5" orient="auto"><path d="M0,0 L7,2.5 L0,5" fill="none" stroke="#39d2c0" stroke-width="1"/></marker>
    </defs>
    <rect x="28" y="28" width="724" height="84" rx="8" fill="rgba(247,120,186,.02)" stroke="rgba(247,120,186,.12)" stroke-dasharray="4 2"/>
    <text x="40" y="41" fill="#f778ba" font-size="8" font-weight="600" letter-spacing=".1em" opacity=".7">SOURCE CONTROL</text>
    <rect x="28" y="128" width="724" height="84" rx="8" fill="rgba(210,153,34,.02)" stroke="rgba(210,153,34,.12)" stroke-dasharray="4 2"/>
    <text x="40" y="141" fill="#d29922" font-size="8" font-weight="600" letter-spacing=".1em" opacity=".7">EDGE / LOAD BALANCING</text>
    <rect x="28" y="228" width="724" height="84" rx="8" fill="rgba(188,140,255,.02)" stroke="rgba(188,140,255,.12)" stroke-dasharray="4 2"/>
    <text x="40" y="241" fill="#bc8cff" font-size="8" font-weight="600" letter-spacing=".1em" opacity=".7">MESSAGE QUEUE / ASYNC</text>
    <rect x="28" y="328" width="724" height="84" rx="8" fill="rgba(57,210,192,.02)" stroke="rgba(57,210,192,.12)" stroke-dasharray="4 2"/>
    <text x="40" y="341" fill="#39d2c0" font-size="8" font-weight="600" letter-spacing=".1em" opacity=".7">APPLICATION SERVICES</text>
    <rect x="28" y="428" width="724" height="84" rx="8" fill="rgba(248,81,73,.02)" stroke="rgba(248,81,73,.12)" stroke-dasharray="4 2"/>
    <text x="40" y="441" fill="#f85149" font-size="8" font-weight="600" letter-spacing=".1em" opacity=".7">DATA STORES</text>
    <line x1="390" y1="94" x2="390" y2="154" stroke="#6e7681" stroke-width="1.2" marker-end="url(#topo_8063)" opacity=".6"/>
    <text x="398" y="124" fill="#6e7681" font-size="7" font-family="'JetBrains Mono',monospace" opacity=".7">webhook</text>
    <line x1="390" y1="194" x2="390" y2="254" stroke="#6e7681" stroke-width="1.2" marker-end="url(#topo_8063)" opacity=".6"/>
    <line x1="390" y1="294" x2="219" y2="354" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <line x1="219" y1="354" x2="333" y2="394" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <line x1="333" y1="354" x2="447" y2="394" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <line x1="447" y1="354" x2="561" y2="394" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <line x1="561" y1="354" x2="390" y2="94" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" stroke-dasharray="4 3" opacity=".5"/>
    <text x="479" y="222" fill="#6e7681" font-size="7" font-family="'JetBrains Mono',monospace" opacity=".7">post comments</text>
    <line x1="219" y1="394" x2="333" y2="454" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <line x1="219" y1="394" x2="447" y2="454" stroke="#6e7681" stroke-width="1" marker-end="url(#topo_8063)" opacity=".5"/>
    <rect class="svg-node" x="340" y="54" width="100" height="40" rx="6" fill="rgba(247,120,186,.06)" stroke="rgba(247,120,186,.3)"/>
    <text x="390" y="71" fill="#f778ba" font-size="10" font-weight="600" text-anchor="middle">üêô GitHub / GitLab</text>
    <text x="390" y="84" fill="#6e7681" font-size="8" text-anchor="middle">PR webhooks</text>
    <rect class="svg-node" x="340" y="154" width="100" height="40" rx="6" fill="rgba(210,153,34,.06)" stroke="rgba(210,153,34,.3)"/>
    <text x="390" y="171" fill="#d29922" font-size="10" font-weight="600" text-anchor="middle">üåê Gateway</text>
    <text x="390" y="184" fill="#6e7681" font-size="8" text-anchor="middle">webhook validation</text>
    <rect class="svg-node" x="340" y="254" width="100" height="40" rx="6" fill="rgba(188,140,255,.06)" stroke="rgba(188,140,255,.3)"/>
    <text x="390" y="271" fill="#bc8cff" font-size="10" font-weight="600" text-anchor="middle">üì® Kafka</text>
    <text x="390" y="284" fill="#6e7681" font-size="8" text-anchor="middle">pr.opened ¬∑ pr.updated</text>
    <rect class="svg-node" x="169" y="354" width="100" height="40" rx="6" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="219" y="371" fill="#39d2c0" font-size="10" font-weight="600" text-anchor="middle">üéØ Review Orchestrat‚Ä¶</text>
    <text x="219" y="384" fill="#6e7681" font-size="8" text-anchor="middle">spin up ‚Üí review ‚Üí post</text>
    <rect class="svg-node" x="283" y="354" width="100" height="40" rx="6" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="333" y="371" fill="#39d2c0" font-size="10" font-weight="600" text-anchor="middle">üì¶ Clone + Index</text>
    <text x="333" y="384" fill="#6e7681" font-size="8" text-anchor="middle">ephemeral sandbox</text>
    <rect class="svg-node" x="397" y="354" width="100" height="40" rx="6" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="447" y="371" fill="#39d2c0" font-size="10" font-weight="600" text-anchor="middle">üîç Lint + Analyze</text>
    <text x="447" y="384" fill="#6e7681" font-size="8" text-anchor="middle">static analysis</text>
    <rect class="svg-node" x="511" y="354" width="100" height="40" rx="6" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="561" y="371" fill="#39d2c0" font-size="10" font-weight="600" text-anchor="middle">ü§ñ AI Review</text>
    <text x="561" y="384" fill="#6e7681" font-size="8" text-anchor="middle">LLM per-file analysis</text>
    <rect class="svg-node" x="283" y="454" width="100" height="40" rx="6" fill="rgba(248,81,73,.06)" stroke="rgba(248,81,73,.3)"/>
    <text x="333" y="471" fill="#f85149" font-size="10" font-weight="600" text-anchor="middle">üêò PostgreSQL</text>
    <text x="333" y="484" fill="#6e7681" font-size="8" text-anchor="middle">reviews ¬∑ config</text>
    <rect class="svg-node" x="397" y="454" width="100" height="40" rx="6" fill="rgba(248,81,73,.06)" stroke="rgba(248,81,73,.3)"/>
    <text x="447" y="471" fill="#f85149" font-size="10" font-weight="600" text-anchor="middle">‚ö° Redis</text>
    <text x="447" y="484" fill="#6e7681" font-size="8" text-anchor="middle">job state ¬∑ cache</text>
  </svg>
</div>

    <div class="comp-grid">
      <div class="comp-card">
        <h4>üîî Webhook Gateway <span class="tag" style="background:rgba(63,185,80,.15);color:var(--accent-green)">INGESTION</span></h4>
        <ul>
          <li>Receives GitHub/GitLab/Bitbucket webhooks</li>
          <li>HMAC signature validation</li>
          <li>ACK in &lt;50ms, enqueue event</li>
          <li>Deliberately "dumb" ‚Äî no business logic</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üì¨ Event Queue <span class="tag" style="background:rgba(210,153,34,.15);color:var(--accent-orange)">BUFFER</span></h4>
        <ul>
          <li>Kafka / Redpanda</li>
          <li>Decouples ingestion from processing</li>
          <li>Absorbs spikes (shock absorber)</li>
          <li>Ordered per-repo for consistency</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üß† Review Orchestrator <span class="tag" style="background:rgba(248,81,73,.15);color:var(--accent-red)">CORE</span></h4>
        <ul>
          <li>Consumes events from queue</li>
          <li>Spins up sandbox per review</li>
          <li>Coordinates: clone ‚Üí context ‚Üí lint ‚Üí LLM ‚Üí post</li>
          <li>Manages model cascade routing</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üì¶ Sandbox (Cloud Run) <span class="tag" style="background:rgba(188,140,255,.15);color:var(--accent-purple)">EPHEMERAL</span></h4>
        <ul>
          <li>Isolated container per review</li>
          <li>Clones repo, runs linters, executes AI scripts</li>
          <li>Double-sandboxed: container + jailkit</li>
          <li>Destroyed after review completes</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üîç Context Engine <span class="tag" style="background:rgba(57,210,192,.15);color:var(--accent-cyan)">CORE</span></h4>
        <ul>
          <li>Builds "case file" for the review</li>
          <li>Code graph (dependencies, call sites)</li>
          <li>Linked issues (Jira/Linear/GitHub)</li>
          <li>Past PRs, team learnings, coding guidelines</li>
          <li>Vector search via LanceDB</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üîß Linter Orchestrator <span class="tag" style="background:rgba(227,179,65,.15);color:var(--accent-yellow)">ANALYSIS</span></h4>
        <ul>
          <li>40+ static analysis tools (ESLint, Rubocop, etc.)</li>
          <li>Auto-detects language ‚Üí runs relevant linters</li>
          <li>Results fed to LLM for synthesis</li>
          <li>Runs in sandbox alongside code</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>ü§ñ LLM Review Engine <span class="tag" style="background:rgba(88,166,255,.15);color:var(--accent-blue)">AI</span></h4>
        <ul>
          <li>Model cascade: fast model ‚Üí complex model</li>
          <li>Summary generation, file-by-file review</li>
          <li>Verification pass (reduce hallucinations)</li>
          <li>Conversational replies in PR threads</li>
        </ul>
      </div>
      <div class="comp-card">
        <h4>üí¨ Platform Adapter <span class="tag" style="background:rgba(63,185,80,.15);color:var(--accent-green)">OUTPUT</span></h4>
        <ul>
          <li>Posts review comments back to PR</li>
          <li>Abstracts GitHub/GitLab/Bitbucket APIs</li>
          <li>Summary comment + inline file comments</li>
          <li>Handles rate limits per platform</li>
        </ul>
      </div>
    </div>

    <div class="sub">Flow 1: PR Opened ‚Üí Full Review</div>
<div class="svg-diagram" data-anim>
  <span class="dia-title">Questions I'd Ask</span>
  <svg viewBox="0 0 560 436" xmlns="http://www.w3.org/2000/svg" style="font-family:'DM Sans',sans-serif">
    <defs><marker id="a5425" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6" fill="none" stroke="#6e7681" stroke-width="1.2"/></marker></defs>
    <rect class="svg-node" x="131" y="40" width="298" height="44" rx="7" fill="rgba(248,81,73,.06)" stroke="rgba(248,81,73,.3)"/>
    <text x="280" y="68" fill="#f85149" font-size="12" font-weight="600" text-anchor="middle">GitHub</text>
    <line x1="280" y1="84" x2="280" y2="138" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a5425)"/>
    <text x="296" y="100" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">validate HMAC, enqueue</text>
    <rect class="svg-node" x="130" y="140" width="301" height="44" rx="7" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="280" y="168" fill="#39d2c0" font-size="12" font-weight="600" text-anchor="middle">Gateway</text>
    <line x1="280" y1="184" x2="280" y2="222" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a5425)"/>
    <rect class="svg-node" x="133" y="224" width="295" height="44" rx="7" fill="rgba(88,166,255,.06)" stroke="rgba(88,166,255,.3)"/>
    <text x="280" y="242" fill="#58a6ff" font-size="12" font-weight="600" text-anchor="middle">Kafka</text>
    <text x="280" y="258" fill="#6e7681" font-size="9" text-anchor="middle">pr.opened</text>
    <line x1="280" y1="268" x2="280" y2="370" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a5425)"/>
    <text x="296" y="284" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">1. Gatekeeper: filter noise (merge commits, bot PRs, generated fi</text>
    <text x="296" y="300" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">~40% of events filtered out here ‚Üí saves compute</text>
    <text x="296" y="316" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">2. Spin up Sandbox (Cloud Run container)</text>
    <text x="296" y="332" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">Clone repo (shallow clone, only needed branches)</text>
    <rect class="svg-node" x="112" y="372" width="337" height="44" rx="7" fill="rgba(210,153,34,.06)" stroke="rgba(210,153,34,.3)"/>
    <text x="280" y="400" fill="#d29922" font-size="12" font-weight="600" text-anchor="middle">Review Orchestrator</text>
  </svg>
</div>

    <div class="sub">Flow 2: Dev Replies to Comment</div>
<div class="svg-diagram" data-anim>
  <span class="dia-title">Flow 2: Dev Replies to Comment</span>
  <svg viewBox="0 0 560 420" xmlns="http://www.w3.org/2000/svg" style="font-family:'DM Sans',sans-serif">
    <defs><marker id="a1009" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6" fill="none" stroke="#6e7681" stroke-width="1.2"/></marker></defs>
    <rect class="svg-node" x="131" y="40" width="298" height="44" rx="7" fill="rgba(248,81,73,.06)" stroke="rgba(248,81,73,.3)"/>
    <text x="280" y="68" fill="#f85149" font-size="12" font-weight="600" text-anchor="middle">GitHub</text>
    <line x1="280" y1="84" x2="280" y2="122" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a1009)"/>
    <rect class="svg-node" x="130" y="124" width="301" height="44" rx="7" fill="rgba(57,210,192,.06)" stroke="rgba(57,210,192,.3)"/>
    <text x="280" y="152" fill="#39d2c0" font-size="12" font-weight="600" text-anchor="middle">Gateway</text>
    <line x1="280" y1="168" x2="280" y2="206" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a1009)"/>
    <rect class="svg-node" x="133" y="208" width="295" height="44" rx="7" fill="rgba(88,166,255,.06)" stroke="rgba(88,166,255,.3)"/>
    <text x="280" y="236" fill="#58a6ff" font-size="12" font-weight="600" text-anchor="middle">Kafka</text>
    <line x1="280" y1="252" x2="280" y2="354" stroke="#6e7681" stroke-width="1.3" marker-end="url(#a1009)"/>
    <text x="296" y="268" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">1. Load prior review context from DB</text>
    <text x="296" y="284" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">2. Parse dev&#x27;s reply: question? feedback? preference?</text>
    <text x="296" y="300" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">3. If feedback ‚Üí store as Learning in LanceDB</text>
    <text x="296" y="316" fill="#d29922" font-size="9" font-family="'JetBrains Mono',monospace">4. Generate response ‚Üí post reply comment</text>
    <rect class="svg-node" x="112" y="356" width="337" height="44" rx="7" fill="rgba(210,153,34,.06)" stroke="rgba(210,153,34,.3)"/>
    <text x="280" y="384" fill="#d29922" font-size="12" font-weight="600" text-anchor="middle">Review Orchestrator</text>
  </svg>
</div>

    <div class="callout say">"The most architecturally interesting piece is the Context Engine ‚Äî how we assemble the 'case file' that makes reviews codebase-aware rather than diff-only. That's what I'd like to deep-dive first. Second, the sandboxed execution model for running untrusted code safely."</div>
  </div>
</div>

<!-- P4 -->
<div class="phase" id="p4">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase4)">04</span>
    <span class="phase-title">Deep Dives</span><span class="phase-time">25‚Äì30 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">

    <!-- DD1 -->
    <div id="dd-pipeline">
    <div class="sub">Deep Dive 1: Review Pipeline & Context Engine (~12 min)</div>

    <div class="callout goal"><strong>The core challenge:</strong> A diff without context is noise. "Line 42 changed" means nothing unless you know it's called by 15 other files, the linked Jira ticket asked for something different, and the team's convention is to handle errors a specific way. Assembling the RIGHT context ‚Äî not all context ‚Äî within 90 seconds is the hardest problem.</div>

    <p style="color:var(--text-muted);margin:12px 0;font-size:13.5px;line-height:1.6;"><strong style="color:var(--text)">Context Layers (assembled in parallel)</strong></p>

    <table>
      <thead><tr><th>Layer</th><th>Source</th><th>What It Provides</th><th>Retrieval Method</th></tr></thead>
      <tbody>
        <tr><td>Diff</td><td>Git</td><td>What changed: added/removed/modified lines per file</td><td>git diff (in sandbox)</td></tr>
        <tr><td>Code Graph</td><td>AST Parser</td><td>Who calls this function? What depends on this module? Downstream impact.</td><td>Build lightweight dependency graph on clone. Tree-sitter for multi-language AST.</td></tr>
        <tr><td>Co-change History</td><td>Git log</td><td>Files that historically change together with the modified files.</td><td>git log --follow analysis</td></tr>
        <tr><td>Linked Issues</td><td>Jira/Linear/GitHub</td><td>The "why" ‚Äî what was the developer trying to accomplish?</td><td>Parse PR description for issue refs, fetch via API</td></tr>
        <tr><td>Past PRs</td><td>LanceDB</td><td>Similar changes, past review feedback on related code.</td><td>Semantic search over PR embeddings</td></tr>
        <tr><td>Team Learnings</td><td>LanceDB</td><td>Preferences from past feedback ("we use 2-space indent", "always check null here").</td><td>Filtered by repo_id, semantic match to current diff</td></tr>
        <tr><td>Coding Guidelines</td><td>.coderabbit.yaml</td><td>Explicit rules: naming conventions, error handling, API patterns.</td><td>Direct config load</td></tr>
        <tr><td>Linter Results</td><td>40+ tools</td><td>Static analysis findings: type errors, security issues, style violations.</td><td>Run in sandbox, parse output</td></tr>
      </tbody>
    </table>

    <div class="callout decision"><strong>Why LanceDB for vector search?</strong> We need to search over millions of PR interactions across 100K repos ‚Äî past reviews, learnings, similar code patterns. LanceDB is an embedded vector DB that can run inside the sandbox or on the worker node, avoiding network round-trips to a centralized vector DB. It supports massive tables with fast upsert for real-time ingestion. Tradeoff: not as feature-rich as Pinecone/Weaviate, but the embedding + search happening co-located with the review worker minimizes latency.</div>

    <p style="color:var(--text-muted);margin:12px 0;font-size:13.5px;line-height:1.6;"><strong style="color:var(--text)">Model Cascade Strategy</strong></p>

    <div class="schema"><span class="comment">‚îÄ‚îÄ Step 1: File Classification (fast model, ~2 sec) ‚îÄ‚îÄ</span>
For each changed file, classify:
  <span class="pk">TRIVIAL</span>   ‚Üí config changes, formatting, comments-only
              ‚Üí skip LLM review OR one-liner ("looks good")
  <span class="pk">MODERATE</span>  ‚Üí standard logic, well-tested patterns
              ‚Üí review with <span class="fk">fast model</span> (Haiku/Sonnet)
  <span class="pk">COMPLEX</span>   ‚Üí security-sensitive, concurrency, architectural
              ‚Üí review with <span class="fk">premium model</span> (Opus/Sonnet)

<span class="comment">‚îÄ‚îÄ Step 2: Batched Review (parallel, ~30-60 sec) ‚îÄ‚îÄ</span>
Each file group reviewed in parallel:
  Context = diff + code graph neighbors + linter results + guidelines
  Prompt = "Review this change. Consider: [context]. Team prefers: [learnings]."

<span class="comment">‚îÄ‚îÄ Step 3: Verification (agent, ~15-30 sec) ‚îÄ‚îÄ</span>
For each non-trivial finding, verification agent:
  Generates a shell script to check the claim
  e.g., "Is function X really unused?" ‚Üí grep -r "functionX" --include="*.ts"
  Runs script in sandbox ‚Üí confirm or retract
  <span class="pk">This step cuts false positives by ~40%</span>

<span class="comment">‚îÄ‚îÄ Cost Savings ‚îÄ‚îÄ</span>
Without cascade: all files ‚Üí premium model = ~$0.15/review
With cascade: 40% trivial + 40% fast + 20% premium = ~$0.02/review
<span class="pk">~7.5√ó cost reduction</span></div>

    <div class="callout decision"><strong>Why a verification pass instead of just better prompting?</strong> LLMs hallucinate. A model might say "this function is never called" when it actually IS called from a file not included in context. The verification agent PROVES claims by running actual code analysis (grep, ast-grep) in the sandbox. This is the difference between "AI opinion" and "verified finding." Tradeoff: adds 15-30 seconds to review time, but dramatically improves trust and accuracy.</div>

    <div class="callout tip"><strong>The Gatekeeper Pattern:</strong> ~40% of webhook events are noise ‚Äî merge commits, bot PRs (Dependabot), auto-generated files (package-lock.json), and CI re-runs. The gatekeeper filters these BEFORE spinning up a sandbox, saving ~40% of compute. Rules are configurable per repo in <code>.coderabbit.yaml</code> (path filters, ignore patterns).</div>
    </div>

    <!-- DD2 -->
    <div id="dd-sandbox">
    <div class="sub">Deep Dive 2: Sandboxed Code Execution (~8 min)</div>

    <div class="callout goal"><strong>The core challenge:</strong> We're cloning and executing code from customer repositories. This code is untrusted ‚Äî it could be buggy, incomplete, or malicious. The linters might run untrusted plugins (ESLint configs). The verification agent generates and executes scripts. All of this must be completely isolated.</div>

    <p style="color:var(--text-muted);margin:12px 0;font-size:13.5px;line-height:1.6;"><strong style="color:var(--text)">Sandbox Architecture (Cloud Run)</strong></p>

    <ul class="items">
      <li><strong>Layer 1: Cloud Run container</strong> ‚Äî each review gets its own container instance. Auto-scaled based on queue depth. Torn down after review completes. Minimal IAM permissions (no access to other customer data).</li>
      <li><strong>Layer 2: gVisor microVM</strong> ‚Äî Cloud Run's second-gen execution environment provides hardware-level isolation. Each container runs in its own microVM.</li>
      <li><strong>Layer 3: Jailkit + cgroups</strong> ‚Äî within the container, linter and script execution runs in a jailed process with restricted filesystem access (only the cloned repo) and CPU/memory limits via cgroups.</li>
      <li><strong>Network:</strong> Outbound network is blocked by default. Only allowlisted domains (package registries for linter plugin install, issue tracker APIs) are permitted.</li>
      <li><strong>Lifetime:</strong> Container exists for the duration of the review (60-90 seconds). All data destroyed on termination. No persistent storage.</li>
    </ul>

    <div class="callout decision"><strong>Why Cloud Run over Kubernetes or Firecracker?</strong> Cloud Run gives us per-request container scaling with zero idle cost, built-in gVisor isolation, and managed infrastructure. Kubernetes would require managing our own node pool and scaling logic. Firecracker (like AWS Lambda) is more isolated but less flexible (no arbitrary binaries for linters). Tradeoff: Cloud Run has cold-start latency (~2-5s for first container), but we keep a minimum instance count to mitigate this during business hours.</div>

    <div class="schema"><span class="comment">‚îÄ‚îÄ Sandbox Lifecycle ‚îÄ‚îÄ</span>

1. <span class="highlight">Spin up</span>: Cloud Run instance starts (~2s warm, ~5s cold)
2. <span class="highlight">Clone</span>:  git clone --depth=1 --branch=<PR branch> (~3-10s)
              + git fetch origin <base branch> --depth=1
3. <span class="highlight">Index</span>:  Build code graph with tree-sitter AST parsing (~2-5s)
4. <span class="highlight">Lint</span>:   Run relevant linters in jailed process (~5-15s)
5. <span class="highlight">Review</span>: LLM calls with assembled context (~30-60s)
6. <span class="highlight">Verify</span>: AI-generated scripts run in jail (~10-20s)
7. <span class="highlight">Post</span>:   Comments posted to PR via platform API (~2s)
8. <span class="highlight">Destroy</span>: Container terminated, all data gone

<span class="comment">Total: 60-90 seconds end-to-end</span></div>
    </div>

    <!-- DD3 -->
    <div id="dd-learning">
    <div class="sub">Deep Dive 3: Learning & Feedback Loop (~5 min)</div>

    <div class="callout goal"><strong>The core challenge:</strong> CodeRabbit must get SMARTER over time for each team. If a dev says "we always handle errors with Result types, not exceptions," every future review should know this. This is a per-repo memory system.</div>

    <p style="color:var(--text-muted);margin:12px 0;font-size:13.5px;line-height:1.6;"><strong style="color:var(--text)">Learning Sources</strong></p>

    <table>
      <thead><tr><th>Source</th><th>How It's Captured</th><th>How It's Applied</th></tr></thead>
      <tbody>
        <tr><td>Chat Feedback</td><td>Dev replies to review comment with correction/preference. NLP classifies as "learning."</td><td>Stored in LanceDB as an embedding. Retrieved via semantic search when similar code is reviewed.</td></tr>
        <tr><td>.coderabbit.yaml</td><td>Explicit rules: "enforce 2-space indent," "flag any use of eval()."</td><td>Injected directly into review prompt. Deterministic, always applied.</td></tr>
        <tr><td>Coding Agent Rules</td><td>Import from Cursor/Copilot rules files (.cursorrules, etc.)</td><td>Parsed and included as coding guidelines.</td></tr>
        <tr><td>Review Outcomes</td><td>Track which comments get resolved vs. dismissed by developers.</td><td>Down-weight comment patterns that are frequently dismissed.</td></tr>
      </tbody>
    </table>

    <div class="schema"><span class="comment">‚îÄ‚îÄ LanceDB: Learnings Table ‚îÄ‚îÄ</span>
<span class="table-name">learnings</span>
  <span class="pk">id</span>               <span class="type">UUID</span>
  <span class="fk">repo_id</span>          <span class="type">UUID</span>
  <span class="fk">org_id</span>           <span class="type">UUID (learnings can be org-wide or repo-specific)</span>
  content          <span class="type">TEXT ("We prefer Result types over exceptions")</span>
  embedding        <span class="type">VECTOR(1536)</span>
  source           <span class="type">ENUM (chat_feedback, yaml_import, auto_inferred)</span>
  confidence       <span class="type">FLOAT (0-1, increases with repeated confirmation)</span>
  created_at       <span class="type">TIMESTAMP</span>
  last_applied_at  <span class="type">TIMESTAMP</span>

<span class="comment">‚îÄ‚îÄ At review time: ‚îÄ‚îÄ</span>
<span class="comment">1. Embed the current diff</span>
<span class="comment">2. Query learnings WHERE repo_id=X, top 10 by semantic similarity</span>
<span class="comment">3. Inject into review prompt as "Team preferences"</span></div>

    <div class="callout decision"><strong>Why not fine-tune a per-customer model?</strong> Fine-tuning is expensive, slow (hours per update), and doesn't adapt in real-time. A retrieval-based approach (embed learnings ‚Üí inject at review time) updates instantly when a developer gives feedback, costs nothing to maintain, and works across model upgrades. Tradeoff: retrieval can miss relevant learnings if the embedding doesn't capture the relationship. We mitigate this with explicit yaml rules as a deterministic fallback.</div>
    </div>

    <!-- DD4 -->
    <div id="dd-data">
    <div class="sub">Deep Dive 4: Data Model & Storage Summary (~5 min)</div>

    <table>
      <thead><tr><th>Data</th><th>Store</th><th>Access Pattern</th><th>Retention</th></tr></thead>
      <tbody>
        <tr><td>Webhook Events</td><td>Kafka</td><td>Ordered per-repo. Consumed by review workers.</td><td>7 days (replay window)</td></tr>
        <tr><td>Review Results</td><td>PostgreSQL</td><td>Per PR: summary, comments, status. Query by repo+PR.</td><td>Permanent (customer data)</td></tr>
        <tr><td>Learnings</td><td>LanceDB</td><td>Semantic search by repo_id + embedding similarity.</td><td>Permanent (grows over time)</td></tr>
        <tr><td>Code Graph Cache</td><td>Redis</td><td>Per-repo dependency graph. Invalidated on new PR.</td><td>TTL: 1 hour</td></tr>
        <tr><td>Repo Metadata</td><td>PostgreSQL</td><td>Installation config, .coderabbit.yaml, connected integrations.</td><td>Permanent</td></tr>
        <tr><td>Customer Code</td><td>EPHEMERAL (sandbox)</td><td>Cloned on demand, destroyed after review.</td><td>0 ‚Äî never persisted</td></tr>
        <tr><td>Linter Results</td><td>In-memory (sandbox)</td><td>Generated and consumed within single review.</td><td>0 ‚Äî destroyed with sandbox</td></tr>
        <tr><td>PR Embeddings</td><td>LanceDB</td><td>Semantic search for "similar past PRs."</td><td>90 days rolling</td></tr>
      </tbody>
    </table>

    <div class="callout decision"><strong>The critical security property:</strong> Customer source code is NEVER stored persistently. It exists only in the ephemeral sandbox container for the duration of the review (60-90 seconds), then is destroyed. LLM queries are in-memory only with zero retention. Only review RESULTS (comments, summaries) and LEARNINGS (preferences) are persisted. This is the core trust proposition.</div>
    </div>

  </div>
</div>

<!-- P5 -->
<div class="phase" id="p5">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase5)">05</span>
    <span class="phase-title">Cross-Cutting Concerns</span><span class="phase-time">10‚Äì12 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">
    <div class="sub">Failure Scenarios</div>
    <div class="failure-row"><span class="scenario">GitHub rate limits our comment posting</span><span class="mitigation">Exponential backoff with jitter. Batch comments into fewer API calls (one review comment with multiple inline notes vs. separate calls). Cache auth tokens, use GitHub App installation tokens (higher rate limit than OAuth).</span></div>
    <div class="failure-row"><span class="scenario">LLM provider is down/slow</span><span class="mitigation">Multi-provider fallback: primary ‚Üí secondary LLM. If all providers down, post partial review (linter results only) with "AI review pending" note. Retry from queue when provider recovers.</span></div>
    <div class="failure-row"><span class="scenario">Sandbox OOM (huge repo)</span><span class="mitigation">Memory limits per container (4GB). Shallow clone with depth=1. If repo exceeds limit, review only changed files without full code graph. Degrade gracefully: diff-only review is still useful.</span></div>
    <div class="failure-row"><span class="scenario">Malicious code in repo</span><span class="mitigation">Triple sandbox isolation (Cloud Run + gVisor + jailkit). No outbound network. CPU/memory cgroups. Even if code exploits a linter, it can't escape the sandbox or access other customers' data.</span></div>
    <div class="failure-row"><span class="scenario">Webhook replay storm (GitHub retries)</span><span class="mitigation">Idempotency: each webhook has a delivery ID. Deduplicate in the gateway using a Redis set with 1h TTL. If we see the same delivery ID twice, ACK without re-enqueuing.</span></div>
    <div class="failure-row"><span class="scenario">Review is wrong (false positive)</span><span class="mitigation">Developer replies "this is incorrect." Learning system captures the correction. Verification agent reduces future false positives. Review quality metrics track dismissal rate per comment type.</span></div>

    <div class="sub">Scalability</div>
    <table>
      <thead><tr><th>At Scale</th><th>What Breaks</th><th>Mitigation</th></tr></thead>
      <tbody>
        <tr><td>10√ó (5M PRs/day)</td><td>Cloud Run concurrent container limit. LanceDB index size per customer. Kafka throughput.</td><td>Multi-region Cloud Run pools. Shard LanceDB by org_id. Kafka partition per top-100 active repos.</td></tr>
        <tr><td>100√ó (50M PRs/day)</td><td>LLM API rate limits become binding constraint. Code graph building at scale. Cost per review must decrease.</td><td>Self-hosted model inference for cost control. Incremental code graph updates (don't rebuild from scratch). Aggressive model cascade ‚Äî push 60%+ to cheapest tier.</td></tr>
      </tbody>
    </table>

    <div class="sub">Security & Compliance</div>
    <ul class="items">
      <li><strong>SOC 2 Type II & GDPR:</strong> Customer code never persisted. LLM queries zero-retention. Audit log of all data access.</li>
      <li><strong>Webhook HMAC:</strong> Every webhook validated against shared secret. Invalid signatures rejected at gateway (403).</li>
      <li><strong>Tenant isolation:</strong> Each review runs in its own container. No shared filesystem. No cross-customer data leakage possible ‚Äî even in memory.</li>
      <li><strong>No training on customer code:</strong> Contractual and technical guarantee. LLM providers configured with zero data retention agreements.</li>
    </ul>

    <div class="sub">Observability</div>
    <ul class="items">
      <li><strong>Review pipeline:</strong> End-to-end latency (webhook ‚Üí comments posted). Stage breakdown (clone, lint, LLM, post). Error rate by stage.</li>
      <li><strong>Quality metrics:</strong> Comment dismissal rate, learning adoption rate, false positive rate (tracked via dev reactions).</li>
      <li><strong>Cost:</strong> LLM tokens per review, model cascade distribution (% trivial/moderate/complex), cost per review trending.</li>
      <li><strong>Alerting:</strong> Review latency p99 > 120s, webhook queue depth > 10K, sandbox OOM rate > 1%, comment posting error rate > 5%.</li>
    </ul>
  </div>
</div>

<!-- P6 -->
<div class="phase" id="p6">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--phase6);color:var(--bg)">06</span>
    <span class="phase-title">Wrap-Up & Evolution</span><span class="phase-time">3‚Äì5 min</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"/></svg>
  </div>
  <div class="phase-body">
    <div class="callout say">"To summarize: the architecture is an event-driven pipeline with three layers ‚Äî a thin webhook gateway that ACKs in 50ms, a Kafka buffer that absorbs traffic spikes, and a fleet of ephemeral Cloud Run sandboxes that execute reviews in isolation. The core intellectual property is the Context Engine, which assembles a multi-layered 'case file' for each review: diff + code graph + linked issues + past PRs + team learnings + linter results. A model cascade routes files to the appropriate LLM tier (trivial ‚Üí skip, moderate ‚Üí fast model, complex ‚Üí premium model), cutting costs ~7.5√ó without quality loss. A verification agent proves claims by running actual code analysis in the sandbox, reducing false positives by ~40%. The learning system captures team feedback via LanceDB embeddings and applies it to future reviews, making the system smarter over time. Customer code is never persisted ‚Äî it exists only in the ephemeral sandbox for 60-90 seconds."</div>

    <div class="sub" id="evolution">What I'd Build Next</div>
    <table>
      <thead><tr><th>Extension</th><th>Why It Matters</th><th>Architecture Impact</th></tr></thead>
      <tbody>
        <tr><td>Auto-Fix PRs</td><td>Don't just comment ‚Äî create a fix PR with one click</td><td>Agent generates code changes in sandbox. Creates branch + commits via GitHub API. Requires higher trust bar ‚Äî verified fixes only.</td></tr>
        <tr><td>IDE Integration (deep)</td><td>Review before PR is even opened</td><td>Local LSP-like service running lightweight version of the review pipeline. No sandbox needed ‚Äî code is already local.</td></tr>
        <tr><td>Cross-PR Impact Analysis</td><td>Detect architectural drift across many PRs over weeks</td><td>Longitudinal analysis over PR embeddings. Weekly "codebase health" reports. Requires persistent code graph (beyond ephemeral).</td></tr>
        <tr><td>Custom Model Fine-Tuning</td><td>Enterprise customers with unique codebases</td><td>Fine-tune smaller model on customer's past reviews. Serve from dedicated inference endpoint. Higher accuracy, lower latency.</td></tr>
        <tr><td>Security-Focused Tier</td><td>Deep security analysis (SAST/DAST level)</td><td>Longer review budget (5 min instead of 90s). Run dynamic analysis (actually execute tests). Requires beefier sandbox with network simulation.</td></tr>
      </tbody>
    </table>

    <div class="callout tip"><strong>Closing framing:</strong> This design is defined by ONE principle: context is everything. A diff without context is noise; a diff with the right context is a senior engineer's review. Every architectural decision ‚Äî the context engine, the code graph, the learning system, the verification agent ‚Äî exists to give the LLM the information a human reviewer would have after years on the team. The model cascade and ephemeral sandboxes make it economically viable and secure. The result: reviews that understand your codebase, not just your diff.</div>
  </div>
</div>


<!-- P7 -->
<div class="phase" id="p7">
  <div class="phase-header" onclick="this.closest('.phase').classList.toggle('collapsed')">
    <span class="phase-number" style="background:var(--accent-cyan)">07</span>
    <span class="phase-title">Interview Q&amp;A</span><span class="phase-time">Practice</span>
    <svg class="phase-chevron" viewBox="0 0 20 20" fill="currentColor"><path d="M6.3 6.3a1 1 0 011.4 0L10 8.6l2.3-2.3a1 1 0 111.4 1.4l-3 3a1 1 0 01-1.4 0l-3-3a1 1 0 010-1.4z"/></svg>
  </div>
  <div class="phase-body">
    <div class="callout say">"Here are the hardest questions an interviewer would ask about this design, and how to answer them. Each answer demonstrates deep understanding of the tradeoffs, not just surface knowledge."</div>

    <div style="margin:8px 0;">
      <div style="display:flex;gap:10px;align-items:flex-start;margin-bottom:8px">
        <span style="font-family:var(--font-mono);font-size:10px;padding:3px 8px;border-radius:4px;background:rgba(248,81,73,.1);color:var(--accent-red);font-weight:600;flex-shrink:0">Q1</span>
        <p style="color:var(--text);font-size:13.5px;font-weight:600;line-height:1.5;margin:0">How do you handle the cold start problem for the first PR in a new repository?</p>
      </div>
      <div style="display:flex;gap:10px;align-items:flex-start;margin-left:0px">
        <span style="font-family:var(--font-mono);font-size:10px;padding:3px 8px;border-radius:4px;background:rgba(63,185,80,.1);color:var(--accent-green);font-weight:600;flex-shrink:0">A</span>
        <p style="color:var(--text-muted);font-size:13px;line-height:1.65;margin:0">Without repository context, the first review would be generic ‚Äî flagging style issues and obvious bugs but missing domain-specific patterns. We mitigate this with: (1) language and framework detection during clone ‚Äî if it's a Rails app, we apply Ruby/Rails-specific review rules, (2) analysis of existing code patterns ‚Äî even on the first PR, we index the existing codebase to understand naming conventions, test patterns, and architecture, (3) README and config file analysis ‚Äî understanding the project's stated conventions, (4) diff-only focus ‚Äî we review what changed, not the entire codebase, so the review is scoped to the PR's intent. By the 10th PR, the system has learned from developer responses (thumbs up/down on comments, which suggestions were accepted) and the reviews become significantly more relevant. The key metric: false positive rate on the first PR is ~30% (acceptable), dropping to ~10% by the 20th PR.</p>
      </div>
    </div>
    <div style="margin:18px 0;">
      <div style="display:flex;gap:10px;align-items:flex-start;margin-bottom:8px">
        <span style="font-family:var(--font-mono);font-size:10px;padding:3px 8px;border-radius:4px;background:rgba(248,81,73,.1);color:var(--accent-red);font-weight:600;flex-shrink:0">Q2</span>
        <p style="color:var(--text);font-size:13.5px;font-weight:600;line-height:1.5;margin:0">What happens if the AI review hallucinates a bug that doesn't exist?</p>
      </div>
      <div style="display:flex;gap:10px;align-items:flex-start;margin-left:0px">
        <span style="font-family:var(--font-mono);font-size:10px;padding:3px 8px;border-radius:4px;background:rgba(63,185,80,.1);color:var(--accent-green);font-weight:600;flex-shrink:0">A</span>
        <p style="color:var(--text-muted);font-size:13px;line-height:1.65;margin:0">False positives are the biggest threat to adoption ‚Äî if developers dismiss reviews as noise, the tool is worthless. Our mitigation is multi-layered: (1) Static analysis FIRST ‚Äî linting and type checking run before the AI review. If the linter passes, the AI is less likely to flag syntax issues. (2) Confidence scoring ‚Äî each comment includes an internal confidence score. Below a threshold, we either suppress the comment or soften the language (&quot;Consider whether...&quot; vs. &quot;Bug: this will crash&quot;). (3) Verification step ‚Äî after the AI generates review comments, a separate validation pass checks whether the flagged code actually exists in the diff and whether the concern is logically consistent. (4) Feedback loop ‚Äî developer reactions (resolve/dismiss) feed back into per-repository fine-tuning. A comment pattern that's consistently dismissed gets suppressed. The honest answer: hallucinations still happen, which is why the review is always advisory (no blocking PRs on AI review) and every comment links to the specific code line for easy verification.</p>
      </div>
    </div>
  </div>
</div>


</main>
<script>
const observer=new IntersectionObserver(e=>{e.forEach(e=>{if(e.isIntersecting){document.querySelectorAll('nav a').forEach(a=>a.classList.remove('active'));const l=document.querySelector(`nav a[href="#${e.target.id}"]`);if(l)l.classList.add('active')}})},{rootMargin:'-20% 0px -70% 0px'});document.querySelectorAll('[id]').forEach(s=>{if(document.querySelector(`nav a[href="#${s.id}"]`))observer.observe(s)});
</script>
</body>
</html>
